{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维\n",
    "\n",
    "很多机器学习的问题都会涉及到有着几千甚至数百万维的特征的训练实例。这不仅让训练过程变得非常缓慢，同时还很难找到一个很好的解,这就时唯独灾难问题，幸运的是，可以通过一些方法来降低特征维度，将一个十分棘手的问题转变成一个可以较为容易解决的问题，降维不仅能够加快训练速度也便于数据的可视化，毕竟太高维的东西无法想象，但凡事有利必有弊，降维不可避免的会丢失某些信息，当然如果足够幸运的话丢失的恰恰是噪声的话可能会让结果比降维之前更好，这通常不会发生，所有应该先尝试使用原始的数据来训练，如果训练速度太慢的话再考虑使用降维。\n",
    "\n",
    "接下来介绍\n",
    "\n",
    "两种主要的降维方法：投影（projection）和流形学习（Manifold Learning）\n",
    "\n",
    "三种流行的降维技术：主成分分析（PCA），核主成分分析（Kernel PCA）和局部线性嵌入（LLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 投影（projection）\n",
    "在大多数现实生活的问题中，训练实例并不是在所有维度上均匀分布的。许多特征几乎是常数，而其他特征则高度相关。结果，所有训练实例实际上位于（或接近）高维空间的低维子空间内，所以可以尝试将每个训练实例垂直投影到合适的子空间上，这就实现了降维，但是，投影并不总是降维的最佳方法。在很多情况下，子空间可能会扭曲和转动，例如著名的瑞士滚动玩具数据集。\n",
    "\n",
    "### 流形学习（Manifold Learning）\n",
    "流形学习的基本思想是将高维特征空间中的样本分布群“平铺”至一个低维空间，同时能保存原高维空间中样本点之间的局部位置相关信息。原空间中的样本分布可能及其扭曲，平铺之后将更有利于样本之间的距离度量，其距离将能更好地反映两个样本之间的相似性。原空间中相邻比较近的点可能不是同一类点，而相邻较远的点还有可能是同一类，“平铺”至低维空间后就能解决这一问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主成分分析（PCA）\n",
    "主成分分析（Principal Component Analysis）是目前为止最流行的降维算法。首先它找到接近数据集分布的超平面，然后将所有的数据都投影到这个超平面上，那怎么选择正确的超平面，选择保持最大方差的k个轴作为基是最合适的，因为方差越大说明包含的信息越丰富，\n",
    "\n",
    "步骤:\n",
    "1. 特征归一化\n",
    "2. 计算协方差矩阵（协方差矩阵满足对称正定）\n",
    "3. 对协方差矩阵做奇异值分解，得到U\n",
    "4. 从U中取出k列（Ur），得到降维后的Z=X*Ur\n",
    "\n",
    "> 推导: https://www.cnblogs.com/pinard/p/6239403.html\n",
    "http://www.360doc.com/content/17/0822/21/10408243_681335432.shtml\n",
    "~https://wenku.baidu.com/view/4cd290afdd3383c4bb4cd2b7.html?sxts=1535374764838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2) # 将n_components设置为0.0到1.0之间的浮点数，表明您希望保留的方差比率\n",
    "#pca.fit_transform(X)  # 可以使用components_访问每一个主成分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增量\tPCA\n",
    "先前PCA实现的一个问题是它需要在内存中处理整个训练集以便SVD算法运行。幸运的是，有了增量PCA（IPCA）算法：可以将训练集分批，并一次只对一个批量使用IPCA算法。这对大型训练集非常有用，并且可以在线应用PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata \n",
    "mnist = fetch_mldata('MNIST original') \n",
    "X = mnist[\"data\"] \n",
    "#使用np.array_split（）方法的IPCA \n",
    "from sklearn.decomposition import IncrementalPCA \n",
    "n_batches = 100 \n",
    "inc_pca = IncrementalPCA(n_components=154) \n",
    "for X_batch in np.array_split(X, n_batches): \n",
    "    inc_pca.partial_fit(X_batch) \n",
    "X_mnist_reduced = inc_pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机PCA\n",
    "随机PCA是个随机算法，能快速找到接近前d个主成分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\") \n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 核PCA(Kernel PCA)\n",
    "类似svm中的核技巧，使得可以实现复杂的非线性投影降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA \n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04) \n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
