{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机(Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM既可以做分类(SVC)也可以做回归(SVR), SVM又可以分为: 线性可分支持向量机，线性支持向量机，非线性支持向量机。当数据线性可分时通过硬间隔最大化学习分类器，近似线性可分时，通过软间隔最大化学习，线性不可分时，通过核方法和软间隔最大化来学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性可分支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据线性可分时，存在无穷个分离超平面可以将数据正确分类，感知机可以误分类最小的策略学习超平面，而支持向量机通过间隔最大化来学习超平面，简单来说就是使得离超平面最近的点与超平面的距离最大。\n",
    "\n",
    "给定数据集:\n",
    "$$ T={(x^{1},y^{1}),...,(x^{n},y^{n})} $$\n",
    "其中，$x^{i} \\in R^{n}, y \\in \\{+1, -1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么计算点与超平面的距离呢？设超平面h为 $ w^{T}x + b = 0 $\n",
    "1. 假设超平面h上有两个点 $x^{'}, x^{''}$, 那么有 $w^{T}x^{'}=-b, w^{T}x^{''}=-b$\n",
    "2. 由于 $w^{T}(x^{'}-x^{''}=0)$,所以 w垂直于 超平面h\n",
    "3. 则点x到超平面的距离可以定义为 $distance(x,h)= |\\frac{w^{T}}{||w||}(x-x^{'})|=\\frac{1}{||w||}|w^{T}x+b|$\n",
    "其中 $\\frac{w}{||w||}$为超平面h的单位法向量\n",
    "4. 一般在训练集的情况下总能将样本分类正确(由于数据是线性可分的)，那么可以将距离重新定义为: $distance(x^{i},h)= |y^{i}\\frac{w^{T}}{||w||}(x-x^{'})|=y^{i}(\\frac{1}{||w||}w^{T}x+b)$,当数据分类正确时，$y^{i}和w^{T}x+b$符号相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么目标就变: $$argmax_{w,b} = \\{ \\frac{1}{||w||}min_{i}[y^{i}(w^{T}+b)] \\}$$\n",
    "\n",
    "令$y^{i}(w^{T}x+b) \\ge 1$,那么只考虑 $argmax_{w,b}\\frac{1}{||w||}$即可，用带约束拉个朗日乘子法来求解即可，为了求导方便，将式子转化为 $argmin_{w,b}\\frac{1}{||w||^{2}}$,因为最大化$\\frac{1}{||w||}$和最小化$\\frac{1}{||w||^{2}}$是等价的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是支持向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性可分的情况下，与分离超平面最近的样本点的实例即为支持向量，即满足约束条件的等式成立的点，也即 $$y^{i}(w^{T}x^{i}+b) = 0$$\n",
    "对于$y^{i}=+1$的正例来讲，支持向量在超平面 $$H1: y^{i}(w^{T}x^{i}+b)=1$$\n",
    "对于$y^{i}=-1$的负例来讲，支持向量在超平面 $$H2: y^{i}(w^{T}x^{i}+b)=-1$$\n",
    "H1和H2是平行的，之间的距离也就是上面所说的间隔，与法向量有关，值为 $\\frac{2}{||w||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性支持向量机和最大软间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性不可分也就是说某些样本点$(x^{i}, y^{i})$不能满足间隔大于等于1的约束条件，为此，可为每个样本点$(x^{i}, y^{i})$加入一个松弛变量$ \\beta^{i} \\ge 0$,使间隔大于等于1， 这样约束条件变为:\n",
    "$$ y^{i}(w^{T}x^{i}+b) \\ge 1 - \\beta^{i}， \\beta^{i} \\ge 0 $$\n",
    "同时对每一个 $\\beta^{i}$支付一个代价$\\beta^{i}$,目标函数变为:\n",
    "$$argmin_{w,b}\\frac{1}{||w||^{2}} + C\\sum_{j=1}^{N}\\beta^{j}$$\n",
    "$ C \\gt 0 $为惩罚参数，C大时对误分类的惩罚增大，C小时对误分类的惩罚减小。目标函数的含义为：使得间隔尽量大，同时使误分类点尽可能的少，C时两者之间的调和系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非线性支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非线性问题往往不好求解，所以通常采取的方法是进行一个非线性变换，将非线性问题转化为线性问题，通过解变换后的线性问题来求解原来的非线性问题， 用线性分类问题来求解非线性分类问题分为两步:首先将原空间映射到一个新空间，然后在新空间中学习线性分类模型，核方法就是这么一个步骤，其基本思想是通过一个非线性变换 将输入空间($R^{n}$) 对应于一个 特征空间(希尔伯特空间)， 使得在输入空间的 超曲面模型 对应于 特征空间的 超平面模型。核方法主要通过核函数来映射空间\n",
    "$$\\kappa(x_i, x_j) = \\phi(x_i)\\cdot\\phi(x_j)$$\n",
    "常见的核函数:\n",
    "* 线性核函数: $$\\kappa(x,x_i) = x \\cdot x_i$$线性核，主要用于线性可分的情况\n",
    "* 多项式核函数 : $$\\kappa(x, x_i) = ((x\\cdot x_i) + 1)^d$$多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。\n",
    "* 高斯（RBF）核函数 : $$\\kappa(x, x_i) = exp(-\\frac{||x - x_i||^2}{\\delta^2})$$高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内\n",
    "* sigmoid核函数 : $$\\kappa(x, x_i) = tanh(\\eta<x, x_i> + \\theta)$$采用sigmoid核函数，支持向量机实现的就是一种多层神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用Scikit-Learn内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性SVM模型（使用LinearSVC类，超参数C=1，hinge损失函数）来检测Virginica鸢尾花"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 加载数据\n",
    "data = datasets.load_iris()\n",
    "\n",
    "X = data[\"data\"][:, (2,3)] # 只选择长度和宽度属性\n",
    "y = (data[\"target\"]==2).astype(np.float64) # Virginica\n",
    "\n",
    "\n",
    "# 2.预处理数据 和 构造模型\n",
    "svm_clf = Pipeline((\n",
    "    ('scaler', StandardScaler()), # 归一化\n",
    "    ('svm', LinearSVC(C=1, loss='hinge'))\n",
    "))\n",
    "\n",
    "# 3. 训练模型\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 预测\n",
    "svm_clf.predict([[4, 1]]) # 不同于Logistic回归分类器，SVM分类器不会输出每个类别的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用SVC类，SVC(kernel='linear',C=1),但是比较慢，一般不推荐，同样可以使用SGDClassifier类，SGDClassifier(loss='hinge',alpha=1/(m*C))使用随机梯度下降来训练一个svm分类器，可用来处理大数据集或者在线学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性不可分，可以通过添加特征，在某些情况下可以变成线性可分的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "X, y = make_moons(n_samples=1000)\n",
    "\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X, y)\n",
    "\n",
    "poly_svm_clf = Pipeline((\n",
    "    ('poly', PolynomialFeatures(degree=3)), # 多项式特征\n",
    "    ('standard', StandardScaler()), # 归一化\n",
    "    ('svm', LinearSVC(C=1, loss='hinge'))\n",
    "))\n",
    "\n",
    "poly_svm_clf.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = poly_svm_clf.predict(X_te)\n",
    "\n",
    "print(precision_score(y_pred, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加多项式特征很容易实现，不仅仅在SVM，在各种机器学习算法都有不错的表现，但是低次数的多项式不能处理非常复杂的数据集，而高次数的多项式却产生了大量的特征，会使模型变得慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当使用SVM时，可以运用一个被称为“核技巧”（kernel trick）的神奇数学技巧。它可以取得就像你添加了许多多项式，甚至有高次数的多项式，一样好的结果。所以不会大量特征导致的组合爆炸，因为并没有增加任何特征。这个技巧可以用SVC类来实现."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 多项式核\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "    ('standard', StandardScaler()), # 归一化\n",
    "    \n",
    "    # 3阶的多项式核\n",
    "    # 超参数coef0控制了高阶多项式与低阶多项式对模型的影响\n",
    "    ('svm', SVC(kernel='poly', C=5, degree=3, coef0=1)) \n",
    "))\n",
    "# 若模型过拟合，可以减小多项式核的阶数。相反的，如果是欠拟合，可以尝试增大它\n",
    "poly_kernel_svm_clf.fit(X_tr, y_tr)\n",
    "y_pred = poly_kernel_svm_clf.predict(X_te)\n",
    "\n",
    "print(precision_score(y_pred, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 高斯RBF核\n",
    "rbf_kernel_svm_clf = Pipeline((\n",
    "    ('standard', StandardScaler()), # 归一化\n",
    "    \n",
    "    # gamma增大使钟型曲线更窄,导致每个样本的影响范围变得更小,即判定边界最终变得更不规则，在单个样本周围环绕\n",
    "    # 较小的γ值使钟型曲线更宽，样本有更大的影响范围，判定边界最终则更加平滑\n",
    "    ('svm', SVC(kernel='poly', C=.001,gamma=5)) \n",
    "))\n",
    "# 若模型过拟合，可以减小gamma值。相反的，如果是欠拟合，可以尝试gamma值\n",
    "rbf_kernel_svm_clf.fit(X_tr, y_tr)\n",
    "y_pred = rbf_kernel_svm_clf.predict(X_te)\n",
    "\n",
    "print(precision_score(y_pred, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svr = LinearSVR(epsilon=1.5)\n",
    "# 处理非线性回归任务，你可以使用核化的SVM模型\n",
    "from sklearn.svm import SVR\n",
    "svm_poly_reg=SVR(kernel=\"poly\",degree=2,C=100,epsilon=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
